#!/bin/env python
# -*- encoding: utf-8 -*-
import re
import sys
import os
p = (os.path.dirname((os.path.abspath(__file__))))
if p not in sys.path:
    sys.path.append(p)
p = (os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if p not in sys.path:
    sys.path.append(p)
import time
import traceback
from datetime import datetime
from datetime import timedelta
from hive_metastore2.ttypes import NoSuchObjectException
from pyutil.program import metrics2 as metrics
from pyutil.consul.bridge import translate_one
import random
from privilege_util import HiveThriftPrivilegeClient
from privilege_util import Region
Config = {
    "metastore_host": "hive-metastore.m.byted.org",
    "metastore_port": 9083,
    "metastore_psm": "data.olap.hms-py.service.lf",
    "metrics_namespace_prefix": "toutiao.service.thrift.data.olap.hms_cli"
}


def init_metrics(metrics_namespace_prefix=Config['metrics_namespace_prefix']):
    metrics.define_timer("call.success.latency.us", "us", prefix=metrics_namespace_prefix)
    metrics.define_timer("call.error.latency.us", "us", prefix=metrics_namespace_prefix)
    metrics.define_counter("call.success.throughput", "req", prefix=metrics_namespace_prefix)
    metrics.define_counter("call.error.throughput", "req", prefix=metrics_namespace_prefix)
    metrics.define_tagkv("to", ["data.olap.hms"])
    metrics.define_tagkv("method", ["get_partitions",
                                    "get_partition_by_name",
                                    "drop_partition_by_name",
                                    "drop_table",
                                    "get_all_tables",
                                    "get_database",
                                    "get_table",
                                    "get_all_databases"])
    metrics.define_tagkv("from_cluster", ["default"])
    metrics.define_tagkv("to_cluster", ["default"])

def _get_region_from_hms_ip(host):
    if host == Config["metastore_host"]:
        return Region["cn"]
    if host.startswith("10.100") or host.startswith("10.110") or host.startswith("10.120"):
        return Region["va"]
    else:
        return Region["cn"]


class HiveThriftContext(object):
    """
    Context manager for hive metastore client.
    """

    def __init__(self, host=Config["metastore_host"], port=Config["metastore_port"], metastore_psm=Config["metastore_psm"], metrics_namespace_prefix=Config["metrics_namespace_prefix"]):
        self.host = host
        self.port = port
        self.psm = metastore_psm

    def _init_transport(self, host, port):
        try:
            from thrift.transport import TSocket
            from thrift.transport import TTransport
            from thrift.protocol import TBinaryProtocol
            region = _get_region_from_hms_ip(host)
            # Note that this will only work with a CDH release.
            # This uses the thrift bindings generated by the ThriftHiveMetastore service in Beeswax.
            # If using the Apache release of Hive this import will fail.
            from hive_metastore2 import ThriftHiveMetastore
            transport = TSocket.TSocket(
                host,
                port
            )
            transport.setTimeout(60 * 1000)
            transport = TTransport.TBufferedTransport(transport)
            protocol = TBinaryProtocol.TBinaryProtocol(transport)
            transport.open()
            self.transport = transport
            from token_user_reader import get_token, get_user_name_from_token
            # init token related, right after init thrift client
            token = get_token()
            user = get_user_name_from_token()
            return HiveThriftPrivilegeClient(ThriftHiveMetastore.Client(protocol), region, token, user)
        except ImportError as e:
            raise Exception('Could not import Hive thrift library:' + str(e))

    def __enter__(self):
        if self.psm and self.host == "hive-metastore.m.byted.org":
            server_list = translate_one(self.psm)
            if not server_list or len(server_list) == 0:
                raise Exception("no server found for {}".format(self.psm))
            # failure detection
            retry_number = 5
            success = False

            while not success and retry_number > 0:
                server = random.choice(server_list)
                host, port = server[0], server[1]
                try:
                    res = self._init_transport(host, port)
                    if res:
                        return res
                except Exception:
                    pass
                retry_number -= 1

        return self._init_transport(self.host, self.port)

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.transport.close()


class HiveMetastoreClient(object):

    def __init__(self, logger=None, host=Config["metastore_host"], port=Config["metastore_port"], metastore_psm=Config["metastore_psm"], metrics_namespace_prefix=Config["metrics_namespace_prefix"]):
        self.logger = logger
        self.host = host
        self.port = port
        self.metastore_psm = metastore_psm
        self.metrics_namespace_prefix = metrics_namespace_prefix
        init_metrics(metrics_namespace_prefix)


    def get_table_partitions(self, tbl_name, db_name="default", limit=-1):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                parts = client.get_partitions(
                    db_name=db_name,
                    tbl_name=tbl_name,
                    max_parts=limit
                )
                end = time.time()
                self.emit_succss_metric(end - start, client.get_partitions.__name__)
                return parts
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_partitions.__name__)
                raise

    def get_table_simple_partition_infos(self, tbl_name, db_name="default", limit=-1):
        output = list()
        for part in self.get_table_partitions(tbl_name, db_name, limit):
            tmp_result = {
                "partition_value": part.values,
                "last_ddl_time": part.parameters["transient_lastDdlTime"],
                "total_size": part.parameters["totalSize"],
                "num_rows": part.parameters["numRows"]
                }
            output.append(tmp_result)
        return output


    def get_partition_by_name(self, tbl_name, db_name, part_spec):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                part = client.get_partition_by_name(
                    db_name,
                    tbl_name,
                    part_spec
                )
                end = time.time()
                self.emit_succss_metric(end - start, client.get_partition_by_name.__name__)
                return part
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_partition_by_name.__name__)
                raise

    def get_table_partition_names(self, db_name, tbl_name, max_parts=1):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                part = client.get_partition_names(
                    db_name,
                    tbl_name,
                    max_parts
                )
                end = time.time()
                self.emit_succss_metric(end - start, client.get_partition_names.__name__)
                return part
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_partition_names.__name__)
                raise

    def get_table_partition_columns(self, db_name, tbl_name):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                part = client.get_partition_names(
                    db_name,
                    tbl_name,
                    max_parts=1
                )

                end = time.time()
                self.emit_succss_metric(end - start, client.get_partition_names.__name__)

                if len(part) == 0:
                    return []
                else:
                    if "/" in part[0]:
                        columns = [item.split('=', 1)[0] for item in part[0].split('/')]
                        return columns
                    else:
                        return [part[0].split('=', 1)[0]]
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_partition_names.__name__)
                raise

    def hive_partition_ready(self, tbl_name, part_spec, db_name="default"):
        if db_name.strip() == "":
            db_name = "default"
        try:
            parts = self.get_partition_by_name(tbl_name, db_name, part_spec)
            gen_time = datetime.strftime(
                    datetime.fromtimestamp(parts.createTime),
                    "%H:%M"
                    )
            return (True, {
                "gen_time": gen_time
            })
        except NoSuchObjectException as e:
            if "table not found" in e.message:
                raise Exception("table not found")
            # ---- deprecated ------
            #if "incomplete partition name" in e.message:
            #    self.logger.info("NoSuchObjectException: partition_name")
            #    partitions = self.get_table_partitions(tbl_name, db_name)
            #    if partitions:
            #        part_values = [part.values[0] for part in partitions]
            #        if part_spec.split('=')[-1] in part_values:
            #            return (True, {})
            return (False, {})
        except Exception as e:
            if self.logger:
                self.logger.info(traceback.format_exc())
            return (False, {})

    def priest_hive_daily_hourly_partition_ready(self, tbl_name, db_name, partition_name, date_value, hour_value):
        date_name, hour_name = partition_name.replace(' ', '').split(',')
        rt = self.hive_partition_ready(
                tbl_name,
                "{}={}/{}={}".format(
                    date_name,
                    date_value,
                    hour_name,
                    hour_value
                    ), db_name)
        if rt[0]:
            return True
        else:
            return False

    def get_rear_name_offset_combinations(self, name_list, offset_list):
        def combinations(*lists):  # convert [[1,2], [3,4,5]] to [[1,3], [1,4], [1,5]. [2,3], [2,4], [2,5]]
            result = []
            total = reduce(lambda x, y: x * y, map(len, lists))

            for i in xrange(0, total):
                step = total
                temp = []
                for l in lists:
                    step /= len(l)
                    temp.append(l[i / step % len(l)])
                result.append(tuple(temp))
            return result

        rear_name_offset_spec = []
        for i, name in enumerate(name_list):
            rear_name_offset_spec.append([])
            offset_specs = offset_list[i].split('|')
            for offset_spec in offset_specs:
                spec = "{}={}".format(name, offset_spec)
                rear_name_offset_spec[i].append(spec)
        self.logger.debug("rear_name_offset_spec={0}".format(rear_name_offset_spec))
        rear_name_offset_spec = ["/".join(item) for item in combinations(*rear_name_offset_spec)]
        self.logger.debug("new rear_name_offset_spec={0}".format(rear_name_offset_spec))
        return rear_name_offset_spec

    def priest_hive_multiple_partition_ready(self, tbl_name, db_name, partition_name, offset, std_time,
                                             second_partition_is_hour, frequency):
        self.logger.debug(
            "partition_name={}, offset={}, std_time={}, frequency={}".format(partition_name, offset, std_time,
                                                                             frequency))
        name_count = partition_name.count(',')
        offset_count = str(offset).count(',')
        if name_count < 1 or name_count != offset_count:  # 只处理二级及二级以上分区
            return False

        date_name, rear_name = partition_name.replace(' ', '').split(',', 1)
        date_offset, rear_offset = offset.replace(' ', '').split(',', 1)
        if frequency in ['hourly', 'daily_hourly']:  # 基准时间随周期变动 周期是小时时 ${date}代表当天，非小时时 ${date}代表昨天
            time_str = std_time - timedelta(hours=1)
            time_spec = [
                datetime.strftime(time_str - timedelta(days=-int(date_offset)), "%Y%m%d"),
                datetime.strftime(time_str - timedelta(days=-int(date_offset)), "%Y-%m-%d")
            ]
        else:
            time_spec = self._priest_get_part_name("daily", std_time, date_offset)  # 一级分区是天

        if second_partition_is_hour:
            if name_count == 1:
                hour_offset = rear_offset
                rear = ""
            else:
                hour_offset, rear = rear_offset.split(',', 1)

            # 兼容小时级分区为 1,2,3
            hour_num_flag = False
            if db_name in ["i18n_dw", "i18n_dw_sg"] and tbl_name in ["user_action_log_daily",
                                                                     "server_impression_log_daily"]:
                hour_num_flag = True

            if '-' in hour_offset[1:]:  # 1-3 -> 01|02|03
                start, end = hour_offset.split("-")
                hour_start = int(start)
                hour_end = int(end) + 1

                if hour_num_flag:
                    temp_offset = ["{}".format(i) for i in xrange(hour_start, hour_end)]
                    hour_offset = "|".join(temp_offset)
                    rear_offset = "{},{}".format(hour_offset, rear) if rear != "" else hour_offset
                else:
                    temp_offset = ["{0:02}".format(i) for i in xrange(hour_start, hour_end)]
                    hour_offset = "|".join(temp_offset)
                    rear_offset = "{},{}".format(hour_offset, rear) if rear != "" else hour_offset
            elif '|' in hour_offset:  # 1|2|3 -> 01|02|03
                if hour_num_flag:
                    hours = ["{}".format(int(i)) for i in hour_offset.split('|')]
                    hour_offset = "|".join(hours)
                    rear_offset = "{},{}".format(hour_offset, rear) if rear != "" else hour_offset
                else:
                    hours = ["{0:02}".format(int(i)) for i in hour_offset.split('|')]
                    hour_offset = "|".join(hours)
                    rear_offset = "{},{}".format(hour_offset, rear) if rear != "" else hour_offset
            else:
                # 小时级任务单个小时为小时偏移量
                if frequency in ['hourly', 'daily_hourly']:
                    time_str = std_time - timedelta(hours=1 - int(hour_offset)) - timedelta(days=-int(date_offset))
                    time_spec = [
                        datetime.strftime(time_str, "%Y%m%d"),
                        datetime.strftime(time_str, "%Y-%m-%d")
                    ]
                    hour_offset = datetime.strftime(time_str, "%H")

                    # 兼容小时级分区为1，2，3
                    if hour_num_flag:
                        hour_offset = str(int(hour_offset))
                # 天级任务单个小时为小时分区
                else:
                    time_spec = self._priest_get_part_name("daily", std_time, date_offset)
                    hour_offset = hour_offset

                self.logger.info("time_spec={0},hour_offset={1}".format(time_spec, hour_offset))
                rear_offset = "{},{}".format(hour_offset, rear) if rear != "" else hour_offset

        rear_name_offset_spec = self.get_rear_name_offset_combinations(rear_name.split(','), rear_offset.split(','))
        self.logger.debug("rear_name_offset_spec={0}".format(rear_name_offset_spec))

        for spec in rear_name_offset_spec:
            part_spec = "{}={}/{}".format(date_name, time_spec[0], spec)
            part_spec2 = "{}={}/{}".format(date_name, time_spec[1], spec)
            # self.logger.info("part_spec='{0}'".format(part_spec))
            # self.logger.info("part_spec2='{0}'".format(part_spec2))
            try:
                rt = self.hive_partition_ready(
                    tbl_name,
                    part_spec,
                    db_name)
                if rt[0]:
                    continue
                rt1 = self.hive_partition_ready(
                    tbl_name,
                    part_spec2,
                    db_name)
                if not rt1[1]:
                    return False
            except Exception as e:
                self.logger.info(e)
                if "table not found" in e:
                    return False
        if self.logger:
            self.logger.info("Hive Partition Ready: %s", rt[0])
        return True

    def priest_hive_partition_ready(self, tbl_name, db_name, std_time=None, offset=0, partition_name="date",
                                    frequency="daily"):
        rt = True
        partition_values = self._priest_get_part_name(frequency, std_time, offset)
        if frequency == "hourly" and partition_name == "date":
            partition_name = "date_hour"
        for part_value in partition_values:
            part_spec = "{}={}".format(partition_name, part_value)
            if self.logger:
                self.logger.info("Ready: %s %s %s" % (tbl_name, part_spec, db_name))
            try:
                tmp_rt = self.hive_partition_ready(
                    tbl_name=tbl_name,
                    part_spec=part_spec,
                    db_name=db_name
                )
            except Exception as e:
                self.logger.info(e)
                if "table not found" in e:
                    tmp_rt = (False, e)
            if tmp_rt[0] == True:
                break
        else:
            rt = False
        if self.logger:
            self.logger.info("Ready: %s", rt)
        return rt

    def priest_drop_hive_partition(self, tbl_name, db_name, std_time, offset=0, partition_name="date", frequency="daily", delete_data=True):
        partition_values = self._priest_get_part_name(frequency, std_time, offset)
        if frequency == "hourly" and  partition_name == "date":
            partition_name = "date_hour"
        if db_name.strip() == "":
            db_name = "default"
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            for part_value in partition_values:
                part_spec = "{}={}".format(partition_name, part_value)
                if self.logger:
                    self.logger.info(
                        "Drop: %s %s %s" % (tbl_name, part_spec, db_name))
                start = time.time()
                try:
                    client.drop_partition_by_name(
                            db_name=db_name,
                            tbl_name=tbl_name,
                            part_name=part_spec,
                            deleteData=delete_data
                            )
                    end = time.time()
                    self.emit_succss_metric(end - start, client.drop_partition_by_name.__name__)
                    break
                except Exception as e:
                    end = time.time()
                    self.emit_failure_metric(end - start, client.drop_partition_by_name.__name__)
                    self.logger.info(traceback.format_exc())
                    continue

    def priest_drop_hive_multiple_partition(self, tbl_name, db_name, std_time, offset, partition_name,
                                            second_partition_is_hour, frequency, delete_data=True):
        self.logger.debug(
            "drop_hive_multiple_partition, tbl_name={}, db_name={}, std_time={}, offset={}, partition_name={}, second_is_hour={}".format(
                tbl_name, db_name, std_time, offset, partition_name, second_partition_is_hour))
        name_count = partition_name.count(',')
        offset_count = str(offset).count(',')
        if name_count < 1 or name_count != offset_count:  # 只处理二级及二级以上分区
            return False

        date_name, rear_name = partition_name.replace(' ', '').split(',', 1)
        date_offset, rear_offset = offset.replace(' ', '').split(',', 1)
        if frequency == 'hourly':  # 基准时间随周期变动 周期是小时时 ${date}代表当天，非小时时 ${date}代表昨天
            std_time += timedelta(1)
        time_spec = self._priest_get_part_name("daily", std_time, date_offset)  # 一级分区是天
        if second_partition_is_hour:
            if name_count == 1:
                hour_offset = rear_offset
                rear = ""
            else:
                hour_offset, rear = rear_offset.split(',', 1)

            if '-' in hour_offset[1:]:  # 1-3 -> 01|02|03
                start, end = hour_offset.split("-")
                hour_start = int(start)
                hour_end = int(end) + 1
                temp_offset = ["{0:02}".format(i) for i in xrange(hour_start, hour_end)]
                hour_offset = "|".join(temp_offset)
                rear_offset = "{},{}".format(hour_offset, rear) if rear != "" else hour_offset
            elif '|' in hour_offset:  # 1|2|3 -> 01|02|03
                hours = ["{0:02}".format(int(i)) for i in hour_offset.split('|')]
                hour_offset = "|".join(hours)
                rear_offset = "{},{}".format(hour_offset, rear) if rear != "" else hour_offset
            else:
                temp_std_time = std_time - timedelta(hours=1 - int(hour_offset))
                time_spec = self._priest_get_part_name("daily", temp_std_time, date_offset)
                hour_offset = datetime.strftime(temp_std_time, "%H")
                self.logger.info("time_spec={0},hour_offset={1}".format(time_spec, hour_offset))
                rear_offset = "{},{}".format(hour_offset, rear) if rear != "" else hour_offset

        rear_name_offset_spec = self.get_rear_name_offset_combinations(rear_name.split(','), rear_offset.split(','))
        self.logger.debug("rear_name_offset_spec={0}".format(rear_name_offset_spec))

        for spec in rear_name_offset_spec:
            part_spec = "{}={}/{}".format(date_name, time_spec[0], spec)
            part_spec2 = "{}={}/{}".format(date_name, time_spec[1], spec)
            self.logger.info("part_spec='{0}'".format(part_spec))
            self.logger.info("part_spec2='{0}'".format(part_spec2))
            rt = self.drop_partition_by_name(
                tbl_name,
                db_name,
                part_spec)
            if rt:
                continue
            rt1 = self.drop_partition_by_name(
                tbl_name,
                db_name,
                part_spec2)

    def drop_partition_by_name(self, tbl_name, db_name, part_name, delete_data=True):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                client.drop_partition_by_name(
                    db_name=db_name,
                    tbl_name=tbl_name,
                    part_name=part_name,
                    deleteData=delete_data
                )
                end = time.time()
                self.emit_succss_metric(end - start, client.drop_partition_by_name)
            except Exception as e:
                self.logger.info(traceback.format_exc())
                end = time.time()
                self.emit_failure_metric(end - start, client.drop_partition_by_name)
                raise


    def _priest_get_part_name(self, frequency, std_time, offset):
        if frequency == "daily":
            partition_values = [
                datetime.strftime(std_time - timedelta(days=1-int(offset)), "%Y%m%d"),
                datetime.strftime(std_time - timedelta(days=1-int(offset)), "%Y-%m-%d")
            ]
        elif frequency == "hourly":
            partition_values = [
                datetime.strftime(std_time - timedelta(hours=1-int(offset)), "%Y%m%d_%H"),
                datetime.strftime(std_time - timedelta(hours=1-int(offset)), "%Y%m%d%H")
            ]
        return partition_values


    def priest_drop_tmp_table(self, tbl_name, std_time, offset=0, frequency="daily"):
        partition_values = self._priest_get_part_name(frequency, std_time, offset)
        tbl_name = "{}_tmp_{}".format(tbl_name, partition_values[0])
        self.logger.info("Drop tmp table: %s %s" % ( "test", tbl_name))
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                client.drop_table(
                    dbname="test",
                    name=tbl_name,
                    deleteData=True
                )
                end = time.time()
                self.emit_succss_metric(end - start, client.drop_table.__name__)
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.drop_table.__name__)
                raise

    def drop_hive_table(self, tbl_name, db_name="default"):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                client.drop_table(
                    dbname=db_name,
                    name=tbl_name,
                    deleteData=True
                    )
                end = time.time()
                self.emit_succss_metric(start - end, client.drop_table.__name__)
            except Exception:
                end = time.time()
                self.emit_failure_metric(start - end, client.drop_table.__name__)
                raise

    def get_schema(self, db_name, table_name):
        output = list()
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                schema = client.get_schema(db_name, table_name)
                end = time.time()
                self.emit_succss_metric(start - end, client.get_schema.__name__)
            except Exception:
                end = time.time()

                raise
            for field in schema:
                output.append({
                    "name": field.name,
                    "type": field.type,
                    "comment": field.comment
                })
        return output

    def get_schema_names(self, db_name, table_name):
        output = dict()
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                schema = client.get_schema(db_name, table_name)
                for field in schema:
                    output[field.name] = 0
                end = time.time()
                self.emit_succss_metric(end - start, client.get_schema.__name__)
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_schema.__name__)
                raise
        return output


    def get_all_tables(self, db_name):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                tables = client.get_all_tables(db_name)
                end = time.time()
                self.emit_succss_metric(end - start, client.get_all_tables.__name__)
                return tables
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_all_tables.__name__)
                raise

    def get_database(self, db_name):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                table = client.get_database(db_name)
                end = time.time()
                self.emit_succss_metric(end - start, client.get_database.__name__)
                return table
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_database.__name__)
                raise

    def get_table(self, db_name, table_name):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                table = client.get_table(db_name, table_name)
                end = time.time()
                self.emit_succss_metric(end - start, client.get_table.__name__)
                return table
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_table.__name__)
                raise

    def get_all_databases(self):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            start = time.time()
            try:
                databases = client.get_all_databases()
                end = time.time()
                self.emit_succss_metric(end - start, client.get_all_databases.__name__)
                return databases
            except Exception:
                end = time.time()
                self.emit_failure_metric(end - start, client.get_all_databases.__name__)
                raise

    def emit_succss_metric(self, latency, method_name):
        metrics.emit_timer("call.success.latency.us", latency * 1000000, tagkv={"to": "data.olap.hms",
                                                                                "method": method_name,
                                                                                "from_cluster": "default",
                                                                                "to_cluster": "default"},
                           prefix=self.metrics_namespace_prefix)

        metrics.emit_counter("call.success.throughput", 1, tagkv={"to": "data.olap.hms",
                                                                  "method": method_name,
                                                                  "from_cluster": "default",
                                                                  "to_cluster": "default"},
                             prefix=self.metrics_namespace_prefix)

    def emit_failure_metric(self, latency, method_name):
        metrics.emit_timer("call.error.latency.us", latency * 1000000, tagkv={"to": "data.olap.hms",
                                                                              "method": method_name,
                                                                              "from_cluster": "default",
                                                                              "to_cluster": "default"},
                           prefix=self.metrics_namespace_prefix)

        metrics.emit_counter("call.error.throughput", 1, tagkv={"to": "data.olap.hms",
                                                                "method": method_name,
                                                                "from_cluster": "default",
                                                                "to_cluster": "default"},
                             prefix=self.metrics_namespace_prefix)


    def test(self):
        with HiveThriftContext(host=self.host, port=self.port, metastore_psm=self.metastore_psm, metrics_namespace_prefix=self.metrics_namespace_prefix) as client:
            print dir(client)

if __name__ == "__main__":
    # test
    with HiveThriftContext() as client:
        print len(client.get_partition_names("origin_log", "event_log_hourly", -1))
    # test
    client = HiveMetastoreClient()
    print(client.get_table("origin_log", "client_launch_terminate_hourly"))

    # test
    client = HiveMetastoreClient(host="hive-metastore.m.byted.org", port=9083)
    print(client.get_table("origin_log", "client_launch_terminate_hourly"))

    # test va
    client = HiveMetastoreClient(host="10.100.6.56", port=9083)
    print(client.get_table("origin_log", "client_launch_terminate_hourly"))

